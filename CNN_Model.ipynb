{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Influence of technical indicators on the forecasting quality of machine learning models for the prediction of share prices</b></h2>\n",
    "<b>Author:</b> Benedikt Grimus</br>\n",
    "<b>E-Mail:</b> benedikt.grimus@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CNN Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing the required packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate data and mathematical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# save and read files\n",
    "import os\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "# getting historical stock data\n",
    "import yfinance\n",
    "# work with time\n",
    "import datetime\n",
    "import time\n",
    "# create permutations\n",
    "import itertools\n",
    "# scale features and split data into training and testing set\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# CNN model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# error metric\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Download historical stock data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_stock_data(tickers: list[str], start_date: datetime.datetime, end_date:datetime.datetime) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function retrieves historical stock data for a given list of tickers within a specified date range.\n",
    "    It uses the yfinance library to fetch the data and drops unnecessary columns.\n",
    "    \n",
    "    Parameters:\n",
    "    tickers (list[str]): A list of stock ticker symbols for which historical data needs to be fetched.\n",
    "    start_date (datetime.datetime): The start date of the historical data period.\n",
    "    end_date (datetime.datetime): The end date of the historical data period.\n",
    "    \n",
    "    Returns:\n",
    "    list[pd.DataFrame]: A list of pandas DataFrames, each containing historical stock data for a single ticker. \n",
    "    A single data frame contains two columns, the open and the close price and uses the date as the index.\n",
    "    If an error occurs while fetching data for a ticker, it is removed from the list and a message is printed.\n",
    "    \"\"\"\n",
    "    hist_data_all_stocks = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            hist_data_all_stocks.append(yfinance.Ticker(ticker=ticker).history(start=start_date, end=end_date))\n",
    "            hist_data_all_stocks[-1] = hist_data_all_stocks[-1].drop([\"High\",\"Low\",\"Volume\",\"Dividends\",\"Stock Splits\"], axis=1)\n",
    "        except:\n",
    "            tickers.remove(ticker)\n",
    "            print(f\"Error with ticker {ticker}! REMOVED it!\")\n",
    "    return hist_data_all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test | is used for development purposes\n",
    "tickers = [\"AAPL\"]\n",
    "# S&P 500\n",
    "# tickers = [\"AAPL\",\"MSFT\",\"AMZN\",\"NVDA\",\"BRK-B\",\"GOOGL\",\"TSLA\",\"XOM\",\"UNH\",\"META\",\"JNJ\",\"JPM\",\"V\",\"PG\",\"HD\",\"MA\",\"CVX\",\"ABBV\",\"MRK\",\"LLY\",\"AVGO\",\"PEP\",\"KO\",\"PFE\",\"TMO\",\"COST\",\"BAC\",\"CSCO\",\"WMT\",\"MCD\"]\n",
    "# DAX\n",
    "# tickers = [\"SAP.DE\",\"SIE.DE\",\"ALV.DE\",\"DTE.DE\",\"AIR.DE\",\"BAYN.SG\",\"MBG.DE\",\"BAS.DE\",\"MUV2.MI\",\"IFX.DE\",\"DPW.DE\",\"DB1.DE\",\"VOW.DE\",\"BMW.DE\",\"RWE.DE\",\"MRK.DE\",\"DBK.DE\",\"ADS.DE\",\"EOAN.DE\",\"VNA.DE\",\"SHL.DE\",\"DTG.DE\",\"CBK.DE\",\"SY1.DE\",\"PAH3.DE\",\"MTX.DE\",\"HNR1.DE\",\"FRE.DE\",\"BEI.DE\",\"HEN3.DE\"]\n",
    "start_date = datetime.datetime(2020,1,1)\n",
    "end_date = datetime.datetime(2023,1,1)\n",
    "\n",
    "hist_data_all_stocks = get_historical_stock_data(tickers=tickers, start_date=start_date, end_date=end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculate technical indicators</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_technical_indicators(hist_data_all_stocks: list[pd.DataFrame], volatility_window: int = 20, momentum_window: int = 20, ma_window: int = 20) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function calculates technical indicators for a list of historical stock data DataFrames.\n",
    "    It calculates Volatility, Relative Strength Index (RSI), Moving Average Convergence Divergence (MACD), and Momentum.\n",
    "    \n",
    "    Parameters:\n",
    "    hist_data_all_stocks (list[pd.DataFrame]): A list of pandas DataFrames, each containing historical stock data for a single ticker.\n",
    "    volatility_window (int): The window size for calculating volatility. Default is 20.\n",
    "    momentum_window (int): The window size for calculating momentum. Default is 20.\n",
    "    ma_window (int): The window size for calculating moving average. Default is 20.\n",
    "    \n",
    "    Returns:\n",
    "    list[pd.DataFrame]: A list of pandas DataFrames, each containing the original historical stock data with added technical indicators.\n",
    "    \"\"\"\n",
    "    for i in range(len(hist_data_all_stocks)):\n",
    "        #Volatility\n",
    "        hist_data_all_stocks[i][f\"Vol_{volatility_window}\"] = hist_data_all_stocks[i]['Open'].rolling(volatility_window).std()\n",
    "        #RSI\n",
    "        delta = hist_data_all_stocks[i]['Open'].diff()\n",
    "        up = delta.clip(lower=0)\n",
    "        down = -1*delta.clip(upper=0)\n",
    "        ema_up = up.ewm(com=14, adjust=False).mean()\n",
    "        ema_down = down.ewm(com=14, adjust=False).mean()\n",
    "        rs = ema_up/ema_down\n",
    "        hist_data_all_stocks[i]['RSI'] = (100 - (100/(1 + rs)))\n",
    "        #MACD\n",
    "        hist_data_all_stocks[i]['MACD'] = hist_data_all_stocks[i]['Open'].ewm(span=12).mean() - hist_data_all_stocks[i]['Open'].ewm(span=26).mean()\n",
    "        #Momentum\n",
    "        hist_data_all_stocks[i][f'Mom_{momentum_window}'] = hist_data_all_stocks[i]['Open'] - hist_data_all_stocks[i]['Open'].shift(momentum_window)\n",
    "        #Moving Average\n",
    "        hist_data_all_stocks[i][f'MA_{ma_window}'] = hist_data_all_stocks[i]['Open'].rolling(ma_window).mean()\n",
    "        #Drop na values\n",
    "        hist_data_all_stocks[i] = hist_data_all_stocks[i].dropna()\n",
    "        #Rearrange the order of the columns so that the target is at the end\n",
    "        hist_data_all_stocks[i].insert(len(hist_data_all_stocks[i].columns) - 1, \"Close\", hist_data_all_stocks[i].pop('Close'))\n",
    "    return hist_data_all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data_all_stocks = calc_technical_indicators(hist_data_all_stocks=hist_data_all_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Establish stationarity using the first difference method</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of all data frames because it is needed to reverse the first difference method which is used to establish stationarity\n",
    "hist_data_all_stocks_non_stationary = []\n",
    "for i in range(len(hist_data_all_stocks)):\n",
    "    hist_data_all_stocks_non_stationary.append(hist_data_all_stocks[i].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stationary(hist_data_all_stocks: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function makes the time series data stationary by applying the first difference method.\n",
    "    The first difference method calculates the difference between consecutive observations.\n",
    "    This helps to remove trends and seasonality from the data, making it easier to analyze and model.\n",
    "\n",
    "    Parameters:\n",
    "    hist_data_all_stocks (list[pd.DataFrame]): A list of pandas DataFrames, each containing historical stock data for a single ticker.\n",
    "        The data should be indexed by date and contain at least one column representing the stock price.\n",
    "\n",
    "    Returns:\n",
    "    list[pd.DataFrame]: A list of pandas DataFrames, each containing the stationary historical stock data for a single ticker.\n",
    "        The first difference method has been applied to remove trends and seasonality.\n",
    "    \"\"\"\n",
    "    for i in range(len(hist_data_all_stocks)):\n",
    "        hist_data_all_stocks[i] = hist_data_all_stocks[i].diff().dropna()\n",
    "    return hist_data_all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data_all_stocks_stationary = make_stationary(hist_data_all_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check for stationarity using the KPSS test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationary(data: pd.Series, significance: float = 0.05):\n",
    "    \"\"\"\n",
    "    This function tests the stationarity of a given time series data using the KPSS test.\n",
    "    The KPSS test is a statistical test for checking if a time series is stationary around a deterministic trend.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.Series): The time series data to be tested for stationarity. It should be indexed by date.\n",
    "    significance (float): The significance level for the KPSS test. Default is 0.05.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the time series is stationary (p-value >= significance), False otherwise.\n",
    "    \"\"\"\n",
    "    statistic, p_value, n_lags, critical_values = kpss(data)\n",
    "    for key, value in critical_values.items():\n",
    "        if p_value < significance:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all companies and technical indicators\n",
    "for i in range(len(hist_data_all_stocks_stationary)):\n",
    "    for j in range(len(hist_data_all_stocks_stationary[i].columns)):\n",
    "        if test_stationary(hist_data_all_stocks_stationary[i][hist_data_all_stocks_stationary[i].columns[j]]) != True:\n",
    "            print(f\"{tickers[i]} {hist_data_all_stocks_stationary[i].columns[j]} not stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create all possible feature combinations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates all possible feature combinations and adds the Close column as the target column to the data frame\n",
    "feature_combinations = []\n",
    "n_features = hist_data_all_stocks_stationary[0].columns[0:len(hist_data_all_stocks_stationary[0].columns) - 1].tolist()\n",
    "for i in range(len(hist_data_all_stocks_stationary[0].columns) + 1):\n",
    "    for subset in itertools.combinations(n_features, i):\n",
    "        if(len(subset) != 0):\n",
    "            feature_combinations.append(list(subset))\n",
    "            feature_combinations[-1].append(\"Close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assing the lists with the data frames to a single list \n",
    "# A list containing different lists which contain data frames each with a different subset of features and the target variable\n",
    "hist_data_all_stocks_feature_combinations = []\n",
    "for i in range(len(hist_data_all_stocks)):\n",
    "    all_feature_combinations_single_stock = []\n",
    "    for j in range(len(feature_combinations)):\n",
    "        all_feature_combinations_single_stock.append(hist_data_all_stocks_stationary[i][feature_combinations[j]])\n",
    "    hist_data_all_stocks_feature_combinations.append(all_feature_combinations_single_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale between 0 and 1 using MinMaScaler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(hist_data_all_stocks_feature_combinations: list[pd.DataFrame]) -> tuple[list[np.ndarray], list[np.ndarray], list[MinMaxScaler], list[MinMaxScaler]]:\n",
    "    \"\"\"\n",
    "    This function scales the features and target values of a list of data frames using MinMaxScaler.\n",
    "    The scalers are returned to be able to scale the predicted values back.\n",
    "\n",
    "    Parameters:\n",
    "    hist_data_all_stocks_feature_combinations (list[pd.DataFrame]): A list of data frames, each containing a different combination of features and the target variable.\n",
    "        The data frames should have columns representing the features and a 'Close' column representing the target variable.\n",
    "\n",
    "    Returns:\n",
    "    tuple[list[np.ndarray], list[np.ndarray], list[MinMaxScaler], list[MinMaxScaler]]: A tuple containing four lists:\n",
    "        - scaled_features_all_stocks: A list of lists, where each inner list contains the scaled features of the corresponding data frame in the input list.\n",
    "        - scaled_target_all_stocks: A list of lists, where each inner list contains the scaled target values of the corresponding data frame in the input list.\n",
    "        - feature_scaler_all_stocks: A list of MinMaxScaler objects, each corresponding to the scaler used to scale the features of the corresponding data frame.\n",
    "        - target_scaler_all_stocks: A list of MinMaxScaler objects, each corresponding to the scaler used to scale the target values of the corresponding data frame.\n",
    "    \"\"\"\n",
    "    #The scalers are returned to be able to scale the predicted values back\n",
    "    feature_scaler_all_stocks = []\n",
    "    scaled_features_all_stocks = []\n",
    "    target_scaler_all_stocks = []\n",
    "    scaled_target_all_stocks = []\n",
    "    for i in range(len(hist_data_all_stocks_feature_combinations)):\n",
    "        feature_scaler_single_stock = []\n",
    "        scaled_features_single_stock = []\n",
    "        target_scaler_single_stock = []\n",
    "        scaled_target_single_stock = []\n",
    "        for j in range(len(hist_data_all_stocks_feature_combinations[i])):\n",
    "            features = hist_data_all_stocks_feature_combinations[i][j].loc[:,~hist_data_all_stocks_feature_combinations[i][j].columns.isin(['Close'])]\n",
    "            feature_scaler = MinMaxScaler()\n",
    "            scaled_features_single_stock.append(feature_scaler.fit_transform(features))\n",
    "            feature_scaler_single_stock.append(feature_scaler)\n",
    "            \n",
    "            target = hist_data_all_stocks_feature_combinations[i][j].loc[:,hist_data_all_stocks_feature_combinations[i][j].columns.isin(['Close'])]\n",
    "            target_scaler = MinMaxScaler()\n",
    "            scaled_target_single_stock.append(target_scaler.fit_transform(target))\n",
    "            target_scaler_single_stock.append(target_scaler)\n",
    "        \n",
    "        scaled_features_all_stocks.append(scaled_features_single_stock)\n",
    "        feature_scaler_all_stocks.append(feature_scaler_single_stock)\n",
    "        scaled_target_all_stocks.append(scaled_target_single_stock)\n",
    "        target_scaler_all_stocks.append(target_scaler_single_stock)\n",
    "\n",
    "    return scaled_features_all_stocks, scaled_target_all_stocks, feature_scaler_all_stocks, target_scaler_all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_all_stocks, scaled_target_all_stocks, feature_scaler_all_stocks, target_scaler_all_stocks = scale_values(hist_data_all_stocks_feature_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Split into training and testing set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(scaled_features_all_stocks, scaled_target_all_stocks, train_size=0.8) -> list[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    This function splits the scaled features and targets into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    scaled_features_all_stocks (list): A list containing a list for each examined stock, which contains lists with the scaled features for all different feature combinations.\n",
    "    scaled_target_all_stocks (list): A list containing a list for each examined stock, which contains lists with the scaled targets for all different feature combinations.\n",
    "    train_size (float, optional): The proportion of the data to include in the training set. Default is 0.8.\n",
    "    \n",
    "    Returns:\n",
    "    train_test_all_stocks (list): A list containing a list for each stock which contains a list for each feature combination. Each inner list contains four lists: features train, features test, targets train, targets test.\n",
    "    \"\"\"\n",
    "    train_test_all_stocks = []\n",
    "    for i in range(len(scaled_features_all_stocks)):\n",
    "        train_test_single_stock = []\n",
    "        for j in range(len(scaled_features_all_stocks[i])):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(scaled_features_all_stocks[i][j], scaled_target_all_stocks[i][j], train_size=train_size, shuffle=False)\n",
    "            train_test_single_stock.append([x_train, x_test, y_train, y_test])\n",
    "        train_test_all_stocks.append(train_test_single_stock)\n",
    "    return train_test_all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_all_stocks = split_data(scaled_features_all_stocks, scaled_target_all_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bring the training and testing dataset into the right format (samples, CNN_input_vector_size, features)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(x_values: list, y_values: list, CNN_input_vector_size: int = 10):\n",
    "    \"\"\"\n",
    "    Format the dataset for CNN input.\n",
    "\n",
    "    Parameters:\n",
    "    x_values (numpy.ndarray): A numpy array containing the timeseries features.\n",
    "    y_values (numpy.ndarray): A numpy array containing the corresponding target values.\n",
    "    CNN_input_vector_size (int, optional): The size of the input vector for the CNN model. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    dataX (numpy.ndarray): A numpy array containing the input vectors for the CNN model.\n",
    "    dataY (numpy.ndarray): A numpy array containing the corresponding target values for the CNN model.\n",
    "    \"\"\"\n",
    "    dataX, dataY = [], []\n",
    "\n",
    "    for i in range(CNN_input_vector_size, len(x_values)):\n",
    "        dataX.append(x_values[(i - CNN_input_vector_size):i])\n",
    "        dataY.append(y_values[i])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_test_all_stocks)):\n",
    "    for j in range(len(train_test_all_stocks[i])):\n",
    "        train_test_all_stocks[i][j][0], train_test_all_stocks[i][j][2] = format_dataset(train_test_all_stocks[i][j][0], train_test_all_stocks[i][j][2], 10)\n",
    "        train_test_all_stocks[i][j][1], train_test_all_stocks[i][j][3] = format_dataset(train_test_all_stocks[i][j][1], train_test_all_stocks[i][j][3], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Set Shape: X={train_test_all_stocks[i][j][0].shape} Y={train_test_all_stocks[i][j][2].shape}\")\n",
    "print(f\"Testing Set Shape: X={train_test_all_stocks[i][j][1].shape} Y={train_test_all_stocks[i][j][3].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for creating CNN models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CNN_input_vector_size: int, n_features: int) -> Sequential:\n",
    "    \"\"\"\n",
    "    This function creates a Convolutional Neural Network (CNN) model for predicting stock prices.\n",
    "\n",
    "    Parameters:\n",
    "    CNN_input_vector_size (int): The size of the input vector for the CNN model. This represents the number of past time steps used as input.\n",
    "    features (int): The number of features in the input data. This represents the number of technical indicators used as input.\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): A compiled CNN model for predicting stock prices.\n",
    "    \"\"\"\n",
    "    neurons = n_features * CNN_input_vector_size\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(CNN_input_vector_size, n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for saving and loading a trained model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Sequential, ticker: str, id: int) -> None:\n",
    "    \"\"\"\n",
    "    This function saves a trained CNN model to a specified file path.\n",
    "\n",
    "    Parameters:\n",
    "    model (tensorflow.keras.models.Sequential): The trained CNN model to be saved.\n",
    "    ticker (str): The string representing the stock the model was trained on.\n",
    "    id (int): A number used to differentiate between the feature combination that was used to train the model.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    file_path = \"{}{}{}{}{}\".format(os.getcwd(), os.sep, \"Pretrained_CNN_Models\", os.sep,f\"model_{ticker}_{id}.h5\")\n",
    "    model.save(file_path)\n",
    "    \n",
    "def load_pretrained_model(ticker: str, id: int):\n",
    "    \"\"\"\n",
    "    This function loads a pre-trained CNN model from a specified file path.\n",
    "\n",
    "    Parameters:\n",
    "    ticker (str): The string representing the stock the model was trained on.\n",
    "    id (int): A number used to differentiate between the feature combination that was used to train the model.\n",
    "\n",
    "    Returns:\n",
    "    model (tensorflow.keras.models.Sequential): The loaded pre-trained CNN model.\n",
    "    \"\"\"\n",
    "    file_path = \"{}{}{}{}{}\".format(os.getcwd(), os.sep, \"Pretrained_CNN_Models\", os.sep,f\"model_{ticker}_{id}.h5\")\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the CNN models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depending of the number of tickers training takes several hours\n",
    "time_of_training = []\n",
    "number_of_models = len(tickers) * len(feature_combinations)\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 8\n",
    "#Loop through each stock\n",
    "for i in range(len(train_test_all_stocks)):\n",
    "    #Loop through all feature combinations\n",
    "    for j in range(len(train_test_all_stocks[i])):\n",
    "        model_start_time = time.time()\n",
    "        current_model_number = i * len(train_test_all_stocks[i]) + j + 1\n",
    "        model = create_model(train_test_all_stocks[i][j][0].shape[1], train_test_all_stocks[i][j][0].shape[2])\n",
    "        model.fit(train_test_all_stocks[i][j][0], train_test_all_stocks[i][j][2], validation_data=(train_test_all_stocks[i][j][1], train_test_all_stocks[i][j][3]), epochs=epochs, batch_size=batch_size ,verbose=0)\n",
    "        save_model(model, tickers[i], j)\n",
    "        time_of_training.append(time.time() - model_start_time)\n",
    "        expected_remaining_time = np.round(np.mean(time_of_training) * (number_of_models - current_model_number) / 60, 2)\n",
    "        #Print the remaining time\n",
    "        if(number_of_models >= 10):\n",
    "            if(current_model_number % int(number_of_models * 0.1) == 0):\n",
    "                print(f\"Create Model: {current_model_number} of {number_of_models}\")\n",
    "                print(f\"Expected remaining time: {expected_remaining_time}min or {np.round(expected_remaining_time*60, 0)}s Current Ø time per model: {np.round(np.mean(time_of_training),1)}s\")\n",
    "        else:\n",
    "            print(f\"Create Model: {current_model_number} of {number_of_models}\")\n",
    "            print(f\"Expected remaining time: {expected_remaining_time}min or {np.round(expected_remaining_time*60, 0)}s Current Ø time per model: {np.round(np.mean(time_of_training),1)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function for calculating the mean absolute percentage error</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mape(actual: list, predicted: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Absolute Percentage Error (MAPE) between two sets of values.\n",
    "\n",
    "    Parameters:\n",
    "    actual (numpy.ndarray or list): The actual values.\n",
    "    predicted (numpy.ndarray or list): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    float: The Mean Absolute Percentage Error (MAPE) between the actual and predicted values.\n",
    "    \"\"\"\n",
    "    mape = np.mean(np.abs((np.array(actual) - np.array(predicted)) / np.array(actual))) * 100\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create predictions using the models and calculate MAPE and accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_and_y_test = False\n",
    "\n",
    "mape = []\n",
    "pred_rescaled_inversed_difference_all_stocks = []\n",
    "y_test_rescaled_inversed_difference_all_stocks = []\n",
    "y_test_direction_accuracy = []\n",
    "#Loop through all stocks\n",
    "for i in range(len(train_test_all_stocks)):\n",
    "    pred_rescaled_inversed_difference_single_stock = []\n",
    "    y_test_rescaled_inversed_difference_single_stock = []\n",
    "    #Loop through all feature combinations\n",
    "    for j in range(len(train_test_all_stocks[i])):\n",
    "        #Rescale the target variable (Close)\n",
    "        y_test_rescaled = target_scaler_all_stocks[i][j].inverse_transform(train_test_all_stocks[i][j][3])\n",
    "        #Load the model and predict\n",
    "        model = load_pretrained_model(tickers[i], j)\n",
    "        pred = model.predict(train_test_all_stocks[i][j][1], verbose=0)\n",
    "        #Rescale the prediction\n",
    "        pred_rescaled = target_scaler_all_stocks[i][j].inverse_transform(pred)\n",
    "        pred_rescaled_inversed_difference_single_stock_single_model = []\n",
    "        y_test_rescaled_inversed_difference_single_stock_single_model = []\n",
    "        same_direction = 0\n",
    "        #Calculate the accuracy by looping through the predictions\n",
    "        for k in range(len(pred_rescaled)):\n",
    "            if np.sign(pred_rescaled[k]) == np.sign(y_test_rescaled[k]):\n",
    "                same_direction += 1\n",
    "            #Reverse the first difference method\n",
    "            pred_rescaled_inversed_difference_single_stock_single_model.append(hist_data_all_stocks_non_stationary[i]['Close'].iloc[len(hist_data_all_stocks_non_stationary[i]['Close']) - len(pred_rescaled) + k] + pred_rescaled[k])\n",
    "            y_test_rescaled_inversed_difference_single_stock_single_model.append(hist_data_all_stocks_non_stationary[i]['Close'].iloc[len(hist_data_all_stocks_non_stationary[i]['Close']) - len(pred_rescaled) + k] + y_test_rescaled[k])\n",
    "        y_test_direction_accuracy.append(same_direction / len(pred_rescaled))\n",
    "        pred_rescaled_inversed_difference_single_stock.append(pred_rescaled_inversed_difference_single_stock_single_model)\n",
    "        y_test_rescaled_inversed_difference_single_stock.append(y_test_rescaled_inversed_difference_single_stock)\n",
    "        #Calculate the MAPE\n",
    "        mape.append(calc_mape(y_test_rescaled_inversed_difference_single_stock_single_model, pred_rescaled_inversed_difference_single_stock_single_model))\n",
    "    if save_predictions_and_y_test:\n",
    "        pred_rescaled_inversed_difference_all_stocks.append(pred_rescaled_inversed_difference_single_stock)\n",
    "        y_test_rescaled_inversed_difference_all_stocks.append(y_test_rescaled_inversed_difference_single_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the x labels for plotting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create x labels is nothing else than just removing the \"Close\" column from the feature combinations\n",
    "x_labels = []\n",
    "for i in range(len(feature_combinations)):\n",
    "    string = \"\"\n",
    "    for j in range(len(feature_combinations[i])):\n",
    "        if(feature_combinations[i][j] != \"Close\"):\n",
    "            string += feature_combinations[i][j]\n",
    "            if j != len(feature_combinations[i]) - 2:\n",
    "                string += \" + \\n\"\n",
    "    x_labels.append(string)\n",
    "#x_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a data frame containg the accuracy results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_direction = pd.DataFrame(np.array(y_test_direction_accuracy).reshape(-1,len(x_labels)), columns=[x_labels + \" dir. acc.\" for x_labels in x_labels], index=tickers)\n",
    "results_df_direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a data frame containg the MAPE results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mae = pd.DataFrame(np.array(mape).reshape(-1,len(x_labels)), columns=x_labels, index=tickers)\n",
    "results_df_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Merge the data frames and save them as an Excel file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_name = 'CNN_results_test'\n",
    "\n",
    "results_df_final = results_df_mae.join(results_df_direction)\n",
    "results_df_final.to_excel(excel_file_name + \".xlsx\", sheet_name=\"Final Results CNN\")\n",
    "results_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculate the correlations between the MAE and the accuarcy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "for i in range(len(results_df_direction)):\n",
    "    correlations.append(np.corrcoef(results_df_direction.loc[tickers[i],:], results_df_mae.loc[tickers[i],:])[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a data frame containing the correlations and save it as an Excel file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlations = pd.DataFrame(np.array(correlations).reshape(-1,len(tickers)), columns=tickers)\n",
    "df_correlations.to_excel(excel_file_name + \"_Correlations.xlsx\", sheet_name=\"Correlations\")\n",
    "df_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot the MAPE against the direction accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = itertools.cycle([(0,0,0.4),(0,0,0.8),(0.6,0,0),(0.8,0,0)])\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_subplot()\n",
    "ax.set_facecolor([.75,.75,.75])\n",
    "ax.set_title(\"MAPE vs Accuracy\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"MAPE\")\n",
    "for i in range(len(results_df_direction)):\n",
    "    ax.scatter(results_df_mae.loc[results_df_mae.index[i]], results_df_direction.loc[results_df_direction.index[i]], color=next(colors), label=tickers[i])\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
